{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Just a simple Chatbot</h1>\n",
    "<br/>\n",
    "<p><i>    This project was born within the course on Natural Language Processing realized for Coursera by Anna Potapenko, Alexey Zobnin, Anna Kozlova, Sergey Yudin and Andrei Zimovnov for the National Research University Higher School of Economics - I will use Starspace, for each reference see the article 'StarSpace: Embed All The Things!'(arXiv:1709.03856) published by Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, Jason Weston\n",
    "</i>    </p> \n",
    "<p>My basic idea to create a chatbot for the HSE Honor project is to make a model as simple as possible. In the moment it shows that it can work - even just rudimentary - I'm going to implement more complex techniques on it. To create the model I will initially use the Cornell Movie Dialogues as a dataset and I take advantage of some of the functions we learned to use during the course.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = 'data/cornell/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:05<00:00, 16121.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# I use datasets.py functions to read our data \n",
    "max_sentence_len = 25  # this is the sentences max_lenght that we consider   \n",
    "\n",
    "data = readCornellData(dataset_path, max_len = max_sentence_len, fast_preprocessing=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our dataset:  38659 \n",
      "\n",
      "Three lines of our dataset:  [('there', 'where'), ('have fun tonight', 'tons'), ('what good stuff', 'the real you')] \n",
      "\n",
      "The same lines in a more readable form: \n",
      " Q: there \n",
      " A: where\n",
      " Q: have fun tonight \n",
      " A: tons\n",
      " Q: what good stuff \n",
      " A: the real you\n"
     ]
    }
   ],
   "source": [
    "# Now, we just explore the data\n",
    "initial_data_len = len(data)\n",
    "print('Size of our dataset: ', initial_data_len, '\\n')\n",
    "print('Three lines of our dataset: ', data[:3], '\\n')\n",
    "\n",
    "print('The same lines in a more readable form: ')\n",
    "for line in data[:3]:\n",
    "    que, ans = line\n",
    "    print(' Q:', que, '\\n', 'A:', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As indicated in the Project's instructions, the utilities with which we load conversations filter and split them into pairs of questions and answers.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>A very simple model</h3>\n",
    "<p>At the bottom of a hierarchy of complexity we can create a model based on rules. In such a model, we condition our answers to a series of predefined questions. This model could gives an adequate response to certain tasks in which the possible questions and answears are limited and can be predefined in detail. \n",
    "Returning to our example, we can create a database of questions to which certain answers can match. As shown below. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = []\n",
    "answ = []\n",
    "for line in data:\n",
    "    q, a = line\n",
    "    ques.append(q)\n",
    "    answ.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If we ask 'have fun tonight' the chatbot must answear:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tons\n"
     ]
    }
   ],
   "source": [
    "question = 'have fun tonight'\n",
    "idx = ques.index(question)\n",
    "print(answ[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>...of course, this kind of answer is not appropriate for creating a chatbot that speaks in natural language. The model above would be in error as soon as I tried to ask a question not included in our list; and we must consider that the possible questions are endless. Secondly in a Chatbot the possible answers to the same question should be different.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>...where we were with the StackOverflow assistant bot</h2>\n",
    "<p>So, I now re-start from where we stopped off with the StackOverflow assistant bot project. I will create a model in which our sample questions are used as training datasets. Each question is transformed into embeddings, so we can use it to create a vector space of possible questions.</p>\n",
    "<p>Then using mathematical functions, our model can identify the distance (and proximity) with questions never seen before and make a ranking. Inspired by the ruled based model seen above, I will consider a success if our model returns as an appropriate response one of the answear already provided in our dataset. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To create the vector representation I use Starspace that show during the HSE course to be fast and to work quite well. I will use the train mode 4: the questions will be considered examples and the answers will be the corresponding labels. Then first of all I have to create the correct file of data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcofosci/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# prepares the sentences for our training by reducing them to lowercase and removing strange and not useful characters.\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    '''A filter function to prepare our sentences.'''\n",
    "    \n",
    "    GOOD_SYMBOLS_RE = re.compile('[^0-9a-z ]')\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;#+_]')\n",
    "    REPLACE_SEVERAL_SPACES = re.compile('\\s+')\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    sentence = REPLACE_BY_SPACE_RE.sub(' ', sentence)\n",
    "    sentence = GOOD_SYMBOLS_RE.sub('', sentence)\n",
    "    sentence = REPLACE_SEVERAL_SPACES.sub(' ', sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the filter above to all dataset\n",
    "\n",
    "def prepare_data(data):\n",
    "    '''A utility function to prepare all dataset'''\n",
    "\n",
    "    train_data = []\n",
    "    for line in data:\n",
    "        new_line = []\n",
    "        for sentence in line:    \n",
    "            new_line.append(prepare_text(sentence))\n",
    "        train_data.append(new_line)\n",
    "        \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the file for starSpace \n",
    "\n",
    "def prepare_file(train_data, f_out):\n",
    "    '''A function to create the correct datafile for starSpace.'''\n",
    "    \n",
    "    out = open(f_out, 'w', encoding='utf8')\n",
    "    for line in train_data:\n",
    "        que, ans = line\n",
    "        newline = que + '\\t' + ans\n",
    "        print(newline, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I entered the data size in the filename to avoid confusion in my directory with other experiments\n",
    "data_file = 'starspace/data/data_' + str(initial_data_len) + '.tsv'\n",
    "\n",
    "prepare_file(train_data, data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Creation of embeddings with Starspace</h5>\n",
    "<p>At this point I will create the embedding file using Starspace from a terminal. Basic I will use the configuration that we used for the Assignment of the third week</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainMode 4 - output:starspace_4\n",
    "starspace train \\\n",
    "-trainFile 'data/data_38658.tsv' \\\n",
    "-model 'data/train_38658' \\\n",
    "-trainMode 4 \\\n",
    "-lr 0.05 \\\n",
    "-adagrad 1 \\\n",
    "-ngrams 1 \\\n",
    "-epoch 5 \\\n",
    "-verbose 1 \\\n",
    "-similarity cosine \\\n",
    "-minCount 2 \\\n",
    "-fileFormat labelDoc \\\n",
    "-negSearchLimit 10 \\\n",
    "-dim 100 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Start to initialize starspace model.\n",
    "Build dict from input file : data/data_38658.tsv\n",
    "Read 0M words\n",
    "Number of words in dictionary:  7041\n",
    "Number of labels in dictionary: 0\n",
    "Loading data from file : data/data_38658.tsv\n",
    "Total number of examples loaded : 37836\n",
    "Initialized model weights. Model size :\n",
    "matrix : 7041 100\n",
    "Training epoch 0: 0.05 0.01\n",
    "Epoch: 100.0%  lr: 0.040000  loss: 0.090191  eta: <1min   tot: 0h0m9s  (20.0%)\n",
    " ---+++                Epoch    0 Train error : 0.08831500 +++--- ☃\n",
    "Training epoch 1: 0.04 0.01\n",
    "Epoch: 100.0%  lr: 0.030000  loss: 0.031533  eta: <1min   tot: 0h0m17s  (40.0%)\n",
    " ---+++                Epoch    1 Train error : 0.03147708 +++--- ☃\n",
    "Training epoch 2: 0.03 0.01\n",
    "Epoch: 100.0%  lr: 0.020000  loss: 0.020011  eta: <1min   tot: 0h0m25s  (60.0%)\n",
    " ---+++                Epoch    2 Train error : 0.02052677 +++--- ☃\n",
    "Training epoch 3: 0.02 0.01\n",
    "Epoch: 100.0%  lr: 0.010270  loss: 0.015211  eta: <1min   tot: 0h0m33s  (80.0%)\n",
    " ---+++                Epoch    3 Train error : 0.01591401 +++--- ☃\n",
    "Training epoch 4: 0.01 0.01\n",
    "Epoch: 100.0%  lr: 0.000541  loss: 0.012553  eta: <1min   tot: 0h0m41s  (100.0%)\n",
    " ---+++                Epoch    4 Train error : 0.01311882 +++--- ☃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use this function to load embeddings from tsv file\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    '''Loads pre-trained word embeddings from tsv file.'''\n",
    "\n",
    "    embeddings = {}\n",
    "    words = []\n",
    "\n",
    "    for line in open(embeddings_path, encoding='utf-8'):\n",
    "        words = line.strip().split('\\t')\n",
    "        embeddings[words[0]] = np.array(np.float32(words[1:]))\n",
    "\n",
    "    embeddings_dim = len(words) - 1\n",
    "        \n",
    "    return embeddings, embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I load the embeddings that have been created\n",
    "embeddings_file = 'starspace/data/train_38658.tsv'\n",
    "embeddings, embeddings_dim = load_embeddings(embeddings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have created our embeddings file we need a function that converts the questions we will make during the conversation into embeddings. We have already used a similar function during the course: <b>question_to_vec</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim):\n",
    "    '''Transforms a string to an embedding by filtering and averaging word embeddings.'''\n",
    "    \n",
    "    result = np.zeros(dim)\n",
    "    cnt = 0\n",
    "    sentence = question.split(' ')\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in embeddings:\n",
    "            result += embeddings[word]\n",
    "            cnt += 1\n",
    "            \n",
    "    if cnt != 0:\n",
    "        result = result / cnt\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We create the vector space that represents our dataset: in it we couple each index of our questions to the related embeddings. The same index will then be fundamental for coupling the questions to the answers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_dict(data):\n",
    "    '''\n",
    "    Transform questions in embeddings \n",
    "    Return also dictionaries for question and answear\n",
    "    '''\n",
    "    \n",
    "    n_ans = len(data)\n",
    "    idx_to_emb = np.zeros((n_ans, embeddings_dim), dtype=np.float32)\n",
    "    ques = {}\n",
    "    answ = {}\n",
    "\n",
    "    for i, line in enumerate(data):\n",
    "        que, ans = line\n",
    "        idx_to_emb[i, :] = question_to_vec(prepare_text(que), embeddings, embeddings_dim)\n",
    "        ques[i] = que\n",
    "        answ[i] = ans \n",
    "        \n",
    "    return idx_to_emb, ques, answ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_emb, idx_to_que, idx_to_ans = emb_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial  20717  element in data:  ('do you really need these', 'only to see') \n",
      "\n",
      "The corresponding question:  do you really need these\n",
      "The corresponding answer:  only to see \n",
      "\n",
      "The question embeddings:  \n",
      " [-0.00257774  0.01850432  0.00950121 -0.00854143  0.00569524  0.01834482\n",
      " -0.00486118 -0.01803708 -0.01664543  0.01179293  0.00612149  0.01358672\n",
      " -0.00492432  0.003093   -0.01275836  0.01827809  0.01453148 -0.00889496\n",
      "  0.00450172  0.01037154  0.01742019  0.01418392  0.00685231 -0.01177643\n",
      " -0.0228159   0.01592279  0.02278012 -0.00831177  0.01313655  0.01717566\n",
      "  0.01569474 -0.02681505  0.01871077  0.00532841 -0.00797549 -0.0150606\n",
      "  0.00750742 -0.01026774  0.0118776  -0.0206617  -0.00414936 -0.010803\n",
      "  0.0241643   0.00539037 -0.00280713 -0.01279942 -0.00745639  0.0138423\n",
      " -0.00948625  0.0079759  -0.00520654  0.00251925  0.004598    0.01119206\n",
      "  0.00324697  0.02887112 -0.03167317  0.01572986  0.01749413 -0.01249568\n",
      "  0.00294685  0.01866975  0.008182   -0.02027527 -0.01188302  0.02569033\n",
      "  0.03127483 -0.00370672 -0.03816059 -0.0191435   0.00569742 -0.00518401\n",
      "  0.00200899 -0.01505878  0.01568792  0.0201606  -0.02136887 -0.00184938\n",
      "  0.01141711  0.03005838 -0.01108426 -0.02151599 -0.02147086 -0.02014383\n",
      "  0.01831699  0.00448595  0.034015    0.01544819  0.03362063 -0.00850149\n",
      " -0.00256211  0.03957676  0.01163086 -0.0033919   0.01773264 -0.0065345\n",
      "  0.02572377  0.00393503  0.00030429 -0.01686462]\n"
     ]
    }
   ],
   "source": [
    "# an example to understand our data structures\n",
    "i = random.randint(0, len(data))\n",
    "print('Our initial ', i, ' element in data: ', data[i], '\\n')\n",
    "\n",
    "print('The corresponding question: ', idx_to_que[i])\n",
    "print('The corresponding answer: ', idx_to_ans[i], '\\n')\n",
    "\n",
    "print('The question embeddings: ', '\\n', idx_to_emb[i, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we need a function that compare a new question with those within our vector space. I will use the <b>pairwise_distances</b> of the python <b>sklearn</b> module.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "\n",
    "def best_ans(question):\n",
    "    '''find the closest question in our vector space and returns the corresponding answear'''\n",
    "    \n",
    "    question = prepare_text(question)\n",
    "    question_vec = question_to_vec(question, embeddings, embeddings_dim) \n",
    "    best = pairwise_distances_argmin(question_vec.reshape(1, -1), idx_to_emb, metric='cosine')[0]\n",
    "    ans = idx_to_ans[best]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If I ask the question 'i' the model should answer me with the answer 'i' - not always in reality, it depends on the training phase and the over/under fitting degree of our embeddings</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you really need these \n",
      "\n",
      "only to see\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_que[i],'\\n')\n",
    "print(best_ans(idx_to_que[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>But now if I ask a new question that is not in our dataset, I should still get an answer</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no way\n"
     ]
    }
   ],
   "source": [
    "question = 'What about her'\n",
    "\n",
    "if question in idx_to_que.values():\n",
    "    print('---- is in our DATASET ----')\n",
    "else: \n",
    "    print(best_ans(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>A problem with this model</h4>\n",
    "<p>A serious problem with this model is that it will always return the same answer for the same question.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no way\n"
     ]
    }
   ],
   "source": [
    "# using the same question as before\n",
    "print(best_ans(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>An easy solution</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Consider for a moment the vector space of our questions. What happen if we consider a certain number of the nearest vectors instead to take just the closest solution? If, for example, we consider the 5 vectors closest to our question and from time to time we randomly choose the related answer relative to one of those five?</p>\n",
    "\n",
    "<p>In this way, if we have a good dataset and the training phase has produced good results we will still be able to have plausible answers that can vary from time to time. Then, instead of having <b>best_ans</b> we will have the function <b>one_of_best</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def one_of_best (question, n):\n",
    "    '''find the n closest question in our vector space and returns one of the corresponding answear'''\n",
    "    \n",
    "    question = prepare_text(question)\n",
    "    question_vec = question_to_vec(question, embeddings, embeddings_dim) \n",
    "    dist = pairwise_distances(question_vec.reshape(1, -1), idx_to_emb, metric='cosine')[0]\n",
    "    ranks = np.argsort(dist)\n",
    "    one_of = random.choice(ranks[:n])\n",
    "    ranking = ranks[:n]\n",
    "\n",
    "    return idx_to_ans[one_of], ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then we had as best answear:  no way\n"
     ]
    }
   ],
   "source": [
    "print('Then we had as best answear: ', best_ans(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answear to the closest questions: \n",
      "\n",
      "no way\n",
      "get something going there\n",
      "shes dying\n",
      "of course\n",
      "are you\n"
     ]
    }
   ],
   "source": [
    "ans, rank = one_of_best(question, 5)\n",
    "\n",
    "print('Answear to the closest questions: \\n')\n",
    "for x in rank:\n",
    "    print(idx_to_ans[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected answear:  of course\n"
     ]
    }
   ],
   "source": [
    "print('Selected answear: ', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example. We have the question:\n",
      "do you know, i am very strong \n",
      "\n",
      "First answear:  yes you are\n",
      "Second answear: are you persuadable\n"
     ]
    }
   ],
   "source": [
    "question = 'do you know, i am very strong'\n",
    "print('Another example. We have the question:')\n",
    "print(question, '\\n')\n",
    "ans, _ = one_of_best(question, 5)\n",
    "print('First answear: ', ans)\n",
    "ans, _ = one_of_best(question, 5)\n",
    "print('Second answear:', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Further improve</h4>\n",
    "<p>To further improve this model we can enlarge the starting dataset. This operation, in addition to increasing the size of our vector space by improving the ability to recognize the questions, also provides a greater range of possible answers. On the other hand, as possible problems we can incur an excessive weighting of our algorithm during the training phase. However, using Starespace allows us to handle large datasets.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:05<00:00, 15483.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# I extend the dataset by taking all the examples provided by the Cornell Movie Dialogues\n",
    "max_sentence_len = 2000  # the lenght of all sentences is less than 2000    \n",
    "\n",
    "data = readCornellData(dataset_path, max_len = max_sentence_len, fast_preprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our dataset:  221272 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Size of our data\n",
    "new_data_len = len(data)\n",
    "print('Size of our dataset: ', new_data_len, '\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I entered the data size in the filename to avoid confusion in my directory with other experiments\n",
    "data_file = 'starspace/data/data_' + str(new_data_len) + '.tsv'\n",
    "\n",
    "prepare_file(train_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainMode 4 - output:starspace_4\n",
    "starspace train \\\n",
    "-trainFile 'data/data_221272.tsv' \\\n",
    "-model 'data/train_221272' \\\n",
    "-trainMode 4 \\\n",
    "-lr 0.05 \\\n",
    "-adagrad 1 \\\n",
    "-ngrams 1 \\\n",
    "-epoch 5 \\\n",
    "-verbose 1 \\\n",
    "-similarity cosine \\\n",
    "-minCount 2 \\\n",
    "-fileFormat labelDoc \\\n",
    "-negSearchLimit 10 \\\n",
    "-dim 100 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Start to initialize starspace model.\n",
    "Build dict from input file : data/data_221272.tsv\n",
    "Read 4M words\n",
    "Number of words in dictionary:  46560\n",
    "Number of labels in dictionary: 0\n",
    "Loading data from file : data/data_221272.tsv\n",
    "Total number of examples loaded : 220895\n",
    "Initialized model weights. Model size :\n",
    "matrix : 46560 100\n",
    "Training epoch 0: 0.05 0.01\n",
    "Epoch: 100.0%  lr: 0.040000  loss: 0.073188  eta: 0h8m  tot: 0h2m6s  (20.0%)\n",
    " ---+++                Epoch    0 Train error : 0.07301633 +++--- ☃\n",
    "Training epoch 1: 0.04 0.01\n",
    "Epoch: 100.0%  lr: 0.030000  loss: 0.026997  eta: 0h5m  tot: 0h3m55s  (40.0%)\n",
    " ---+++                Epoch    1 Train error : 0.02731198 +++--- ☃\n",
    "Training epoch 2: 0.03 0.01\n",
    "Epoch: 100.0%  lr: 0.020000  loss: 0.018365  eta: 0h3m  tot: 0h5m37s  (60.0%)\n",
    " ---+++                Epoch    2 Train error : 0.01830668 +++--- ☃\n",
    "Training epoch 3: 0.02 0.01\n",
    "Epoch: 100.0%  lr: 0.010000  loss: 0.014678  eta: 0h1m  tot: 0h7m15s  (80.0%)\n",
    " ---+++                Epoch    3 Train error : 0.01454613 +++--- ☃\n",
    "Training epoch 4: 0.01 0.01\n",
    "Epoch: 100.0%  lr: 0.000000  loss: 0.012364  eta: <1min   tot: 0h8m52s  (100.0%)\n",
    " ---+++                Epoch    4 Train error : 0.01259348 +++--- ☃\n",
    "Saving model to file : data/train_221272\n",
    "Saving model in tsv format : data/train_221272.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Some considerations on embeddings provided by starspace</h4>\n",
    "<p>- The initial dataset had <b>37.836 examples</b> loaded and a dictionary of <b>7.041 words</b>. With 100 dimensions for our embeddings, Starspace took a total of 41 seconds for a training of 5 epochs. <i>The final train error was</i>: <b>0.01311882</b></p>\n",
    "<p>- The new dataset have <b>220.895 examples</b> loaded and a dictionary of <b>46.560 words</b>. With 100 dimensions for our embeddings, Starspace took a total of 8 minutes and 52 seconds for a training of 5 epochs. <i>The final train error was</i>: <b>0.01259348</b></p>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I load the embeddings that have been created\n",
    "embeddings_file = 'starspace/data/train_221272.tsv'\n",
    "embeddings, embeddings_dim = load_embeddings(embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the vector space of our new dataset\n",
    "idx_to_emb, idx_to_que, idx_to_ans = emb_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's try now to generate some test answers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five answears to the closest questions: \n",
      "\n",
      "dont think that i want to save myself from any embarrassment from the awkwardness of meeting anna its not that its that you can say certain things easier if youre alone please sandro do try to understand me it would look like i was trying to influence you to force you to control you and that makes me feel uncomfortable\n",
      "yes i know i started to say i started to say joe that\n",
      "to the penny exactly one million dollars in cash\n",
      "a joke is a story with a humorous climax\n",
      "i never joke about money\n"
     ]
    }
   ],
   "source": [
    "question = 'are you joking?'\n",
    "ans, rank = one_of_best(question, 5)\n",
    "\n",
    "print('Five answears to the closest questions: \\n')\n",
    "for x in rank:\n",
    "    print(idx_to_ans[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the question:\n",
      "are you joking? \n",
      "\n",
      "First answear:  a joke is a story with a humorous climax\n",
      "Second answear: i never joke about money\n"
     ]
    }
   ],
   "source": [
    "print('We have the question:')\n",
    "print(question, '\\n')\n",
    "ans, _ = one_of_best(question, 5)\n",
    "print('First answear: ', ans)\n",
    "ans, _ = one_of_best(question, 5)\n",
    "print('Second answear:', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>...a larger dataset</h4>\n",
    "<p>Considering the speed of use of Starspace you can think about changing the parameters of the training phase, playing in particular on the learning rate, on the dimension of our embeddings, on the learning periods. To do this it is advisable to find an efficient performance meter (on it I will work in future).</p> \n",
    "\n",
    "<p>For the moment, I'm content to test the model on a larger dataset: the Open Subtitles with over a million and a half pairs of questions/answers. The use of this dataset allows at least to greatly widen the variety of responses that we have available.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "OpenSubtitles data files:   0%|          | 0/2319 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenSubtitles conversations in data/opensubs/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  17%|█▋        | 403/2319 [00:32<02:35, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Action/2003/602_152466_207871_batoru_rowaiaru_ii_rekuiemu.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  21%|██        | 488/2319 [00:39<02:29, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Action/2004/59_84873_113518_appurushdo.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  52%|█████▏    | 1196/2319 [01:56<01:49, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Comedy/2003/529_124078_171007_how_to_lose_a_guy_in_10_days.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  54%|█████▎    | 1241/2319 [02:02<01:46, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Comedy/2004/2480_226704_299940_little_black_book.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  75%|███████▍  | 1732/2319 [02:54<00:59,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Drama/2000/179_88528_119102_batoru_rowaiaru.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  79%|███████▉  | 1834/2319 [03:04<00:48,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Drama/2002/3265_149497_204017_unfaithful.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  81%|████████  | 1872/2319 [03:07<00:44,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Drama/2003/1723_68784_89159_big_fish.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  83%|████████▎ | 1931/2319 [03:11<00:38, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Drama/2004/146_206647_272090_eternal_sunshine_of_the_spotless_mind.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  89%|████████▉ | 2067/2319 [03:23<00:24, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Family/2001/3935_19508_22105_cats__dogs.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files:  90%|█████████ | 2091/2319 [03:26<00:22, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file data/opensubs/OpenSubtitles/en/Horror/1922/1166_134135_184270_nosferatu_eine_symphonie_des_grauens.xml.gz with errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files: 100%|██████████| 2319/2319 [03:46<00:00, 10.23it/s]\n",
      "100%|██████████| 1648080/1648080 [00:35<00:00, 45889.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now I try to use data from Open Subtitles\n",
    "max_sentence_len = 5000  # the lenght of all sentences is less than 2000    \n",
    "path = 'data/opensubs/' \n",
    "data = readOpensubsData(path, max_len=max_sentence_len, fast_preprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our dataset:  1616544 \n",
      "\n",
      "Three lines of our dataset:  [('right then go straight to the office', 'don t dawdle on the way'), ('don t dawdle on the way', 'don t worry'), ('is your mother here too', 'why are you outside')] \n",
      "\n",
      "The same lines in a more readable form: \n",
      " Q: right then go straight to the office \n",
      " A: don t dawdle on the way\n",
      " Q: don t dawdle on the way \n",
      " A: don t worry\n",
      " Q: is your mother here too \n",
      " A: why are you outside\n"
     ]
    }
   ],
   "source": [
    "# Now, we just explore the data\n",
    "openS_data_len = len(data)\n",
    "print('Size of our dataset: ', openS_data_len, '\\n')\n",
    "print('Three lines of our dataset: ', data[:3], '\\n')\n",
    "\n",
    "print('The same lines in a more readable form: ')\n",
    "for line in data[:3]:\n",
    "    que, ans = line\n",
    "    print(' Q:', que, '\\n', 'A:', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I entered the data size in the filename to avoid confusion in my directory with other experiments\n",
    "data_file = 'starspace/data/data_' + str(openS_data_len) + '.tsv'\n",
    "\n",
    "prepare_file(train_data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainMode 4 - output:starspace_4\n",
    "starspace train \\\n",
    "-trainFile 'data/data_1616544.tsv' \\\n",
    "-model 'data/train_1616544_' \\\n",
    "-trainMode 4 \\\n",
    "-lr 0.05 \\\n",
    "-adagrad 1 \\\n",
    "-ngrams 1 \\\n",
    "-epoch 5 \\\n",
    "-verbose 1 \\\n",
    "-similarity cosine \\\n",
    "-minCount 2 \\\n",
    "-fileFormat labelDoc \\\n",
    "-negSearchLimit 10 \\\n",
    "-dim 100 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Start to initialize starspace model.\n",
    "Build dict from input file : data/data_1616544.tsv\n",
    "Read 23M words\n",
    "Number of words in dictionary:  95764\n",
    "Number of labels in dictionary: 0\n",
    "Loading data from file : data/data_1616544.tsv\n",
    "Total number of examples loaded : 1616048\n",
    "Initialized model weights. Model size :\n",
    "matrix : 95764 100\n",
    "Training epoch 0: 0.05 0.01\n",
    "Epoch: 100.0%  lr: 0.040001  loss: 0.050703  eta: 0h46m  tot: 0h11m34s  (20.0%)\n",
    " ---+++                Epoch    0 Train error : 0.05081936 +++--- ☃\n",
    "Training epoch 1: 0.04 0.01\n",
    "Epoch: 100.0%  lr: 0.030001  loss: 0.027494  eta: 0h31m  tot: 0h22m0s  (40.0%)\n",
    " ---+++                Epoch    1 Train error : 0.02756327 +++--- ☃\n",
    "Training epoch 2: 0.03 0.01\n",
    "Epoch: 100.0%  lr: 0.020001  loss: 0.022612  eta: 0h21m  tot: 0h32m32s  (60.0%)\n",
    " ---+++                Epoch    2 Train error : 0.02258618 +++--- ☃\n",
    "Training epoch 3: 0.02 0.01\n",
    "Epoch: 100.0%  lr: 0.010001  loss: 0.020001  eta: 0h11m  tot: 0h43m33s  (80.0%)\n",
    " ---+++                Epoch    3 Train error : 0.01996631 +++--- ☃\n",
    "Training epoch 4: 0.01 0.01\n",
    "Epoch: 100.0%  lr: 0.000000  loss: 0.018301  eta: <1min   tot: 0h54m10s  (100.0%)\n",
    " ---+++                Epoch    4 Train error : 0.01831765 +++--- ☃\n",
    "Saving model to file : data/train_1616544\n",
    "Saving model in tsv format : data/train_1616544.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>- The Open Subtitles dataset had <b>1.616.048 examples</b> loaded and a dictionary of <b>95.764 words</b>. With 100 dimensions for our embeddings, Starspace took a total of 54 minutes and 10 seconds for a training of 5 epochs. <i>The final train error was</i>: <b>0.01831765</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I load the embeddings that have been created\n",
    "embeddings_file = 'starspace/data/train_1616544.tsv'\n",
    "embeddings, embeddings_dim = load_embeddings(embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the vector space of our new dataset\n",
    "idx_to_emb, idx_to_que, idx_to_ans = emb_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answear to the closest questions: \n",
      "\n",
      "last year i had to drive to lakewood to talk with a client and i went\n",
      "last year i had to drive to lakewood to talk with a client and i went\n",
      "could be fun i m tempted\n",
      "i don t like that that makes me look like a big dufus doesn t it\n",
      "i haven t had a vacation for ages and a few days over there\n"
     ]
    }
   ],
   "source": [
    "question = 'What do you think, about Amazon forest?'\n",
    "ans, rank = one_of_best(question, 5)\n",
    "\n",
    "print('Answear to the closest questions: \\n')\n",
    "for x in rank:\n",
    "    print(idx_to_ans[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the question:\n",
      "What do you think, about Amazon forest? \n",
      "\n",
      "First answear:  last year i had to drive to lakewood to talk with a client and i went\n",
      "Second answear: could be fun i m tempted\n"
     ]
    }
   ],
   "source": [
    "print('We have the question:')\n",
    "print(question, '\\n')\n",
    "ans, _ = one_of_best(question, 5)\n",
    "print('First answear: ', ans)\n",
    "ans, _ = one_of_best(question, 5)\n",
    "print('Second answear:', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I save question embeddings and answears in a pickle file to use it on Telegram NlpHonorBot\n",
    "import pickle\n",
    "pickle.dump(idx_to_emb, open('NlpHonorBot_emb.pkl', 'wb'))\n",
    "pickle.dump(idx_to_ans, open('NlpHonorBot_ans.pkl', 'wb'))\n",
    "\n",
    "# we don't need for the chatbot, but it is also good to save our index of questions for future tests\n",
    "pickle.dump(idx_to_que, open('NlpHonorBot_que.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problems, possible next steps and conclusions</h2>\n",
    "<p>Among the problems of this model, the most evident is that we use pre-packaged phrases for answers: for so many that they can be, they will never be able to return the variability of a real discourse. Furthermore, the language used will follow that of the dataset used for the training phase.</p>\n",
    "\n",
    "<p>Another big problem is that this model has no memory of the statements made, every couple of questions and answers is in itself and does not influence the continuation of the conversation as it happens in natural language.</p>\n",
    "\n",
    "<p>The first problem is partly solveble by implementing a <b>generative model</b> for the creation of responses. This solution leads to a whole series of other problems related to the connection between the vector space of the questions and that of the answers. A good solution should include an <b>encoder-decoder</b> model.</p>\n",
    "\n",
    "<p>The second problem is complex and calls into question some mechanisms - such as the <b>Attention of the sequence to sequence models</b> - that learn and in some way memorize significant parts of the discourse.</p>\n",
    "\n",
    "<p>These are issues that go beyond the time I have available for this project. I will commit myself to implement them in a future projects. I hope this sheet can help other students to understand some of the algorithms developed during the course.</p>\n",
    "\n",
    "<p>I implemented the results of this project in the Telegram chatbot: <b>NlpHonorBot</b>.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
